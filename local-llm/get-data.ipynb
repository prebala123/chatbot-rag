{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Local Vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Google Drive API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "# Set up the credentials and service\n",
    "SCOPES = ['https://www.googleapis.com/auth/documents.readonly']\n",
    "SERVICE_ACCOUNT_FILE = ''\n",
    "DOCUMENT_ID = ''\n",
    "\n",
    "creds = service_account.Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "service = build('docs', 'v1', credentials=creds)\n",
    "document = service.documents().get(documentId=DOCUMENT_ID).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use regex patterns to extract question and answer data from the google doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_qa_from_doc(doc):\n",
    "    content = doc.get('body').get('content')\n",
    "    faq_list = []\n",
    "    current_faq = {}\n",
    "    collecting_answer = False\n",
    "    collecting_question = False\n",
    "\n",
    "    question_patterns = [\n",
    "        re.compile(r'^(Question|Problem|Issue)\\s*:\\s*', re.IGNORECASE)\n",
    "    ]\n",
    "    answer_patterns = [\n",
    "        re.compile(r'^(Answer|Solution)\\s*:\\s*', re.IGNORECASE)\n",
    "    ]\n",
    "\n",
    "    def match_patterns(text, patterns):\n",
    "        for pattern in patterns:\n",
    "            if pattern.match(text):\n",
    "                return pattern\n",
    "        return None\n",
    "\n",
    "    # Loop through every FAQ block and extract relevant information\n",
    "    for element in content:\n",
    "        if 'paragraph' in element:\n",
    "            paragraph = element.get('paragraph')\n",
    "            text = ''.join([run.get('textRun', {}).get('content', '') for run in paragraph.get('elements')]).strip()\n",
    "            italic_found = any(run.get('textRun', {}).get('textStyle', {}).get('italic', False) for run in paragraph.get('elements'))\n",
    "\n",
    "            if text.startswith(\"FAQ #\"):\n",
    "                if 'question' in current_faq and 'answer' in current_faq:\n",
    "                    faq_list.append(current_faq)\n",
    "                current_faq = {\"faq_number\": text}\n",
    "                collecting_answer = False\n",
    "                collecting_question = False\n",
    "            elif text.startswith(\"Title:\"):\n",
    "                current_faq[\"title\"] = text.replace(\"Title:\", \"\").strip()\n",
    "            elif text.startswith(\"Tag:\"):\n",
    "                current_faq[\"tag\"] = text.replace(\"Tag:\", \"\").replace('â€™', '\\'').strip()\n",
    "            elif text.startswith(\"Author:\"):\n",
    "                current_faq[\"author\"] = text.replace(\"Author:\", \"\").strip()\n",
    "            elif match_patterns(text, question_patterns):\n",
    "                if 'question' in current_faq and 'answer' in current_faq:\n",
    "                    faq_list.append(current_faq)\n",
    "                    current_faq = {\"faq_number\": current_faq[\"faq_number\"]}\n",
    "                current_faq[\"question\"] = match_patterns(text, question_patterns).sub(\"\", text).strip()\n",
    "                collecting_answer = False\n",
    "                collecting_question = True\n",
    "            elif match_patterns(text, answer_patterns):\n",
    "                current_faq[\"answer\"] = [match_patterns(text, answer_patterns).sub(\"\", text).strip()]\n",
    "                collecting_answer = True\n",
    "                collecting_question = False\n",
    "            elif collecting_answer:\n",
    "                if italic_found:\n",
    "                    collecting_answer = False\n",
    "                else:\n",
    "                    if text:\n",
    "                        current_faq[\"answer\"].append(text)\n",
    "            elif collecting_question and text:\n",
    "                current_faq[\"question\"] += \" \" + text\n",
    "\n",
    "    # Add the last FAQ entry\n",
    "    if 'question' in current_faq and 'answer' in current_faq:\n",
    "        faq_list.append(current_faq)\n",
    "\n",
    "    # Convert answer lists to single strings\n",
    "    for faq in faq_list:\n",
    "        faq[\"answer\"] = '\\n'.join(faq[\"answer\"])\n",
    "\n",
    "    return faq_list\n",
    "\n",
    "qa_pairs = extract_qa_from_doc(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the FAQs to Document format to store in the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_json_to_documents(json_data):\n",
    "    documents = []\n",
    "    for item in json_data:\n",
    "        metadata = {\n",
    "            'document': item['faq_number'],\n",
    "            'title': item['title'] if 'title' in item else '',\n",
    "            'tag': item['tag'],\n",
    "            'author': item['author'] if 'author' in item else ''\n",
    "        }\n",
    "        page_content = f\"Question: {item['question']}\\nAnswer: {item['answer']}\"\n",
    "        documents.append(Document(metadata=metadata, page_content=page_content))\n",
    "    return documents\n",
    "\n",
    "jsondocs = transform_json_to_documents(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read incidents csv file to get more data and convert to Document format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incidents = pd.read_csv('fixedincidents.csv').dropna(subset=['Solution']).reset_index(drop=True)\n",
    "content = (\n",
    "    'Problem: ' + \n",
    "    incidents['Error Reported'] + \n",
    "    ' \\n\\nSolution: ' + \n",
    "    incidents['Solution']\n",
    ")\n",
    "csvdocs = []\n",
    "n = len(content)\n",
    "for i in range(n):\n",
    "    csvdocs.append(Document(page_content=content[i]))\n",
    "jsondocs = transform_json_to_documents(qa_pairs)\n",
    "docs = jsondocs + csvdocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add all documents into a vectorstore format to be easily accessed by queries for documents with similar embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    collection_name=\"faqs\",\n",
    "    embedding=HuggingFaceEmbeddings(),\n",
    "    persist_directory='./chroma_db',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
